{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Features_for_NLP.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdmlworkshop/Examples/blob/main/Features_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEc0y-LgVS3h"
      },
      "source": [
        "# Tools for NLP\n",
        "\n",
        "There are lots of feature transformations that need to be done on text input to get it to a point that machine learning algorithms can start ptocessing. Spark has placed the most important ones in convienent Feature Transformer calls. \n",
        "\n",
        "Let's look over them before jumping into the spam example\n",
        "\n",
        "\n",
        "\n",
        "ML. It's the newest library that's based on the use of DataFrame, which is basically an RDD[Row] (a Row is just a sequence of untyped objects) with a schema (ie an object that contains information about the colum names, types, metadata...). The fit method which is the method that all the estimators need to implement.\n",
        "\n",
        "Explanation: The ML library uses the notion of Pipeline. A pipeline instance is basically an array of stages (of type PipelineStage), each one of them being either an **Estimator** or a **Transformer** (there are some other types, e.g. Evaluator but I won't get into them here as they are being rare). \n",
        "\n",
        "A Transformer is simply an algorithm that transforms your data, so its main method is `transform(DataFrame)` and it outputs another DataFrame. \n",
        "\n",
        "An Estimator is a an algorithm that produces a Model (a subtype of Transformer). It's basically any block that needs to fit (to train) on data, so it has a function `fit(DataFrame)` that outputs a Transformer. \n",
        "\n",
        "For instance if you want to multiply all your data by $2$, you only need a transformer that implements a transform method that takes your input and multiply it by $2$. \n",
        "If you need to compute the mean and substract it, you need an estimator that fits on the data to compute the mean and outputs a transformer the substracts the mean learned. So any time you use ML, use the fit and transform methods.\n",
        "\n",
        "While Spark ML pipelines have a wide variety of algorithms, you may find yourself wanting additional functionality without having to leave the pipeline model. In Spark MLlib (rdd implementation), this isn’t much of a problem—you can manually implement your algorithm with RDD transformations and keep going from there. For Spark ML pipelines (Dataframes imlementation), the same approach can work, but we lose some of the nicely integrated properties of the pipeline, including the ability to automatically run meta-algorithms, such as cross-validation parameter search.\n",
        "\n",
        "To add your own algorithm to a Spark pipeline, you need to implement either Estimator or Transformer, which implements the PipelineStage interface. For algorithms that don’t require training, you can implement the Transformer interface, and for algorithms with training you can implement the Estimator interface—both in org.apache.spark.ml (both of which implement the base PipelineStage). Note that training is not limited to complicated machine learning models; even the MinMaxScaler requires training to determine the range. If they need training, they must be constructed as Estimator rather than Transformer.\n",
        "\n",
        "## Let start the introductory examples\n",
        "### First as usual with Google Collab, install and init Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHz5i_7ub3sl"
      },
      "source": [
        "!wget -q https://mirrors.netix.net/apache/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar -xzf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "# define some evironement variable diretly with python instruction using the module os\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default-java\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fouo5jXXVS3m"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1b8thOzVS3q"
      },
      "source": [
        "spark = SparkSession.builder.appName('BDML_nlp_tools').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSgpwwo7VS3t"
      },
      "source": [
        "## Tokenizer\n",
        "<p><a href=\"http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\">Tokenization</a> is the process of taking text (such as a sentence) and breaking it into individual terms (usually words).  A simple <a href=\"api/scala/index.html#org.apache.spark.ml.feature.Tokenizer\">Tokenizer</a> class provides this functionality.  The example below shows how to split sentences into sequences of words.</p>\n",
        "\n",
        "<p><a href=\"api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer\">RegexTokenizer</a> allows more\n",
        " advanced tokenization based on regular expression (regex) matching.\n",
        " By default, the parameter &#8220;pattern&#8221; (regex, default: <code>\"\\\\s+\"</code> one or many spaces) is used as delimiters to split the input text.\n",
        " Alternatively, users can set parameter &#8220;gaps&#8221; to false indicating the regex &#8220;pattern&#8221; denotes\n",
        " &#8220;tokens&#8221; rather than splitting gaps, and find all matching occurrences as the tokenization result.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-XxA3MmVS3u"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "#help(RegexTokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjShAMJfVmSX"
      },
      "source": [
        "help(col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QesypCXXVydf"
      },
      "source": [
        "help(udf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOL0ieUIVS3y"
      },
      "source": [
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (0, \"Hi do you heard about Spark?\"),\n",
        "    (1, \"I wish I knew Python and pysaprk before\"),\n",
        "    (2, \"Logistic,regression,models,are,neat\")\n",
        "], [\"id\", \"sentence\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhsik5QwVS31"
      },
      "source": [
        "sentenceDataFrame.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3kLn-qjVS36"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
        "\n",
        "countTokens = udf(lambda words: len(words), IntegerType())\n",
        "\n",
        "tokenized = tokenizer.transform(sentenceDataFrame)\n",
        "tokenized.select(\"sentence\", \"words\")\\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "769jhKw0WJYT"
      },
      "source": [
        "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
        "regexTokenized.select(\"sentence\", \"words\") \\\n",
        "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlSLcBc9jBv9"
      },
      "source": [
        "regexTokenizer.getGaps()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA7wls_ZVS3_"
      },
      "source": [
        "\n",
        "## Stop Words Removal\n",
        "\n",
        "<p><a href=\"https://en.wikipedia.org/wiki/Stop_words\">Stop words</a> are words which\n",
        "should be excluded from the input, typically because the words appear\n",
        "frequently and don&#8217;t carry as much meaning.</p>\n",
        "\n",
        "<p><code>StopWordsRemover</code> takes as input a sequence of strings (e.g. the output\n",
        "of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>) and drops all the stop\n",
        "words from the input sequences. The list of stopwords is specified by\n",
        "the <code>stopWords</code> parameter. Default stop words for some languages are accessible \n",
        "by calling <code>StopWordsRemover.loadDefaultStopWords(language)</code>, for which available \n",
        "options are &#8220;danish&#8221;, &#8220;dutch&#8221;, &#8220;english&#8221;, &#8220;finnish&#8221;, &#8220;french&#8221;, &#8220;german&#8221;, &#8220;hungarian&#8221;, \n",
        "&#8220;italian&#8221;, &#8220;norwegian&#8221;, &#8220;portuguese&#8221;, &#8220;russian&#8221;, &#8220;spanish&#8221;, &#8220;swedish&#8221; and &#8220;turkish&#8221;. \n",
        "A boolean parameter <code>caseSensitive</code> indicates if the matches should be case sensitive \n",
        "(false by default).</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJc55EcgVS4A"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09lIbbIPVS4F"
      },
      "source": [
        "## n-grams\n",
        "\n",
        "An n-gram is a sequence of nn tokens (typically words) for some integer nn. The NGram class can be used to transform input features into nn-grams.\n",
        "\n",
        "<p><code>NGram</code> takes as input a sequence of strings (e.g. the output of a <a href=\"ml-features.html#tokenizer\">Tokenizer</a>).  The parameter <code>n</code> is used to determine the number of terms in each $n$-gram. The output will consist of a sequence of $n$-grams where each $n$-gram is represented by a space-delimited string of $n$ consecutive words.  If the input sequence contains fewer than <code>n</code> strings, no output is produced.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxY9VdjVS4G"
      },
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"hi\", \"do\", \"you\", \"heard\", \"about\", \"spark\"]),\n",
        "    (1, [\"i\", \"wish\", \"i\", \"knew\", \"python\", \"and\", \"pysaprk\", \"before\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N_JIR9EVS4J"
      },
      "source": [
        "_______\n",
        "# Feature Extractors\n",
        "_______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSf6g7rEVS4K"
      },
      "source": [
        "<h2 id=\"tf-idf\">TF-IDF</h2>\n",
        "\n",
        "<p><a href=\"http://en.wikipedia.org/wiki/Tf%E2%80%93idf\">Term frequency-inverse document frequency (TF-IDF)</a> \n",
        "is a feature vectorization method widely used in text mining to reflect the importance of a term \n",
        "to a document in the corpus. Denote a term by $t$ , a document by  d , and the corpus by D.\n",
        "Term frequency $TF(t, d)$ is the number of times that term `$t$` appears in document $d$, while \n",
        "document frequency $DF(t, D)$ is the number of documents that contains term $t$. If we only use \n",
        "term frequency to measure the importance, it is very easy to over-emphasize terms that appear very \n",
        "often but carry little information about the document, e.g. &#8220;a&#8221;, &#8220;the&#8221;, and &#8220;of&#8221;. If a term appears \n",
        "very often across the corpus, it means it doesn&#8217;t carry special information about a particular document.\n",
        "Inverse document frequency is a numerical measure of how much information a term provides:\n",
        "\n",
        "$$ IDF(t, D) = \\log \\frac{|D| + 1}{DF(t, D) + 1} $$\n",
        "\n",
        "where |D| is the total number of documents in the corpus. Since logarithm is used, if a term \n",
        "appears in all documents, its IDF value becomes 0. Note that a smoothing term is applied to avoid \n",
        "dividing by zero for terms outside the corpus. The TF-IDF measure is simply the product of TF and IDF:\n",
        "$$ TFIDF(t, d, D) = TF(t, d) \\cdot IDF(t, D). $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhqhRjL_VS4K"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi do you heard about Spark?\"),\n",
        "    (0.0, \"I wish I knew Python and pysaprk before\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "sentenceData.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blJkywLqVS4M"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v-xZD3ZVS4P"
      },
      "source": [
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "featurizedData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRTpifIvXoDy"
      },
      "source": [
        "featurizedData.select(\"label\", \"rawFeatures\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lumk413wVEbH"
      },
      "source": [
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lftn83NVS4R"
      },
      "source": [
        "## CountVectorizer\n",
        "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
        "\n",
        "During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X9ydZAkVS4S"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uB3iN8EVS4X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}